{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"FTFbmzohkOhi"},"outputs":[],"source":["import numpy as np\n","from tensorflow import keras\n","from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import load_model\n","from nltk.tokenize import word_tokenize\n","import nltk\n","\n","# Fix random seed for reproducibility\n","np.random.seed(7)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_l82PZzskOhl","executionInfo":{"status":"ok","timestamp":1717996599591,"user_tz":-330,"elapsed":5226,"user":{"displayName":"Aritra Paul","userId":"06261948115169749460"}},"outputId":"5aae966e-f699-4c97-d13b-b8958ec80a81"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17464789/17464789 [==============================] - 0s 0us/step\n"]}],"source":["# Load the dataset with a reasonable vocabulary size\n","top_words = 10000  # Adjust as needed\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1BGsnpRvkOhn","executionInfo":{"status":"ok","timestamp":1717996608965,"user_tz":-330,"elapsed":2399,"user":{"displayName":"Aritra Paul","userId":"06261948115169749460"}},"outputId":"0f551b35-8e5b-496b-97d6-55a098025f2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["---review---\n","[1, 6740, 365, 1234, 5, 1156, 354, 11, 14, 5327, 6638, 7, 1016, 2, 5940, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 2, 9363, 1117, 1831, 7485, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 2, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 2, 8564, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 7175, 180, 6, 227, 11, 94, 2494, 2, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 5390, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]\n","---label---\n","1\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n","1641221/1641221 [==============================] - 0s 0us/step\n","---review with words---\n","['the', 'boiled', 'full', 'involving', 'to', 'impressive', 'boring', 'this', 'as', 'murdering', 'naschy', 'br', 'villain', 'and', 'suggestion', 'need', 'has', 'of', 'costumes', 'b', 'message', 'to', 'may', 'of', 'props', 'this', 'and', 'concentrates', 'concept', 'issue', 'skeptical', 'to', \"god's\", 'he', 'is', 'and', 'unfolds', 'movie', 'women', 'like', \"isn't\", 'surely', \"i'm\", 'and', 'to', 'toward', 'in', \"here's\", 'for', 'from', 'did', 'having', 'because', 'very', 'quality', 'it', 'is', 'and', 'starship', 'really', 'book', 'is', 'both', 'too', 'worked', 'carl', 'of', 'and', 'br', 'of', 'reviewer', 'closer', 'figure', 'really', 'there', 'will', 'originals', 'things', 'is', 'far', 'this', 'make', 'mistakes', 'and', 'was', \"couldn't\", 'of', 'few', 'br', 'of', 'you', 'to', \"don't\", 'female', 'than', 'place', 'she', 'to', 'was', 'between', 'that', 'nothing', 'dose', 'movies', 'get', 'are', 'and', 'br', 'yes', 'female', 'just', 'its', 'because', 'many', 'br', 'of', 'overly', 'to', 'descent', 'people', 'time', 'very', 'bland']\n","---label---\n","1\n","Maximum review length: 2697\n","Minimum review length: 70\n"]}],"source":["# Inspect a sample review and its label\n","print('---review---')\n","print(X_train[6])\n","print('---label---')\n","print(y_train[6])\n","\n","# Get word-to-index and index-to-word dictionaries\n","word2id = imdb.get_word_index()\n","id2word = {i: word for word, i in word2id.items()}\n","\n","# Print a review with words\n","print('---review with words---')\n","print([id2word.get(i, ' ') for i in X_train[6]])\n","print('---label---')\n","print(y_train[6])\n","\n","# Determine maximum and minimum review lengths\n","max_review_length = max(len(seq) for seq in (X_train + X_test))\n","min_review_length = min(len(seq) for seq in (X_train + X_test))\n","\n","print('Maximum review length:', max_review_length)\n","print('Minimum review length:', min_review_length)\n","\n","# Pad sequences to a fixed length\n","X_train = pad_sequences(X_train, maxlen=max_review_length)\n","X_test = pad_sequences(X_test, maxlen=max_review_length)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QGHHDUR_kOho","executionInfo":{"status":"ok","timestamp":1717997957712,"user_tz":-330,"elapsed":190053,"user":{"displayName":"Aritra Paul","userId":"06261948115169749460"}},"outputId":"b80a43b4-3286-4dda-8c9f-debd9748b578"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 2697, 128)         1280000   \n","                                                                 \n"," dropout (Dropout)           (None, 2697, 128)         0         \n","                                                                 \n"," lstm (LSTM)                 (None, 128)               131584    \n","                                                                 \n"," dropout_1 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense (Dense)               (None, 1)                 129       \n","                                                                 \n","=================================================================\n","Total params: 1411713 (5.39 MB)\n","Trainable params: 1411713 (5.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/20\n","391/391 [==============================] - 88s 214ms/step - loss: 0.4586 - accuracy: 0.7824 - val_loss: 0.3684 - val_accuracy: 0.8409\n","Epoch 2/20\n","391/391 [==============================] - 76s 194ms/step - loss: 0.3264 - accuracy: 0.8634 - val_loss: 0.4250 - val_accuracy: 0.8062\n","Epoch 3/20\n","391/391 [==============================] - 66s 169ms/step - loss: 0.2478 - accuracy: 0.9028 - val_loss: 0.3204 - val_accuracy: 0.8643\n","Epoch 4/20\n","391/391 [==============================] - 68s 174ms/step - loss: 0.1742 - accuracy: 0.9351 - val_loss: 0.3357 - val_accuracy: 0.8608\n","Epoch 5/20\n","391/391 [==============================] - 66s 169ms/step - loss: 0.1312 - accuracy: 0.9521 - val_loss: 0.3943 - val_accuracy: 0.8566\n","Epoch 6/20\n","391/391 [==============================] - 65s 166ms/step - loss: 0.1048 - accuracy: 0.9627 - val_loss: 0.4530 - val_accuracy: 0.8492\n","Epoch 7/20\n","391/391 [==============================] - 64s 165ms/step - loss: 0.1386 - accuracy: 0.9462 - val_loss: 0.5110 - val_accuracy: 0.7994\n","Epoch 8/20\n","391/391 [==============================] - 65s 165ms/step - loss: 0.1481 - accuracy: 0.9462 - val_loss: 0.4737 - val_accuracy: 0.8597\n","Epoch 9/20\n","391/391 [==============================] - 59s 151ms/step - loss: 0.0837 - accuracy: 0.9708 - val_loss: 0.4886 - val_accuracy: 0.8569\n","Epoch 10/20\n","391/391 [==============================] - 64s 163ms/step - loss: 0.0596 - accuracy: 0.9810 - val_loss: 0.5746 - val_accuracy: 0.8434\n","Epoch 11/20\n","391/391 [==============================] - 59s 151ms/step - loss: 0.0496 - accuracy: 0.9846 - val_loss: 0.6035 - val_accuracy: 0.8517\n","Epoch 12/20\n","391/391 [==============================] - 64s 163ms/step - loss: 0.0409 - accuracy: 0.9869 - val_loss: 0.6485 - val_accuracy: 0.8568\n","Epoch 13/20\n","391/391 [==============================] - 64s 163ms/step - loss: 0.0376 - accuracy: 0.9883 - val_loss: 0.6564 - val_accuracy: 0.8560\n","Epoch 14/20\n","391/391 [==============================] - 59s 151ms/step - loss: 0.0510 - accuracy: 0.9819 - val_loss: 0.6092 - val_accuracy: 0.8514\n","Epoch 15/20\n","391/391 [==============================] - 59s 150ms/step - loss: 0.0718 - accuracy: 0.9751 - val_loss: 0.5648 - val_accuracy: 0.8244\n","Epoch 16/20\n","391/391 [==============================] - 63s 161ms/step - loss: 0.0511 - accuracy: 0.9841 - val_loss: 0.6290 - val_accuracy: 0.8483\n","Epoch 17/20\n","391/391 [==============================] - 64s 163ms/step - loss: 0.0477 - accuracy: 0.9842 - val_loss: 0.5491 - val_accuracy: 0.8181\n","Epoch 18/20\n","391/391 [==============================] - 58s 149ms/step - loss: 0.0287 - accuracy: 0.9916 - val_loss: 0.7660 - val_accuracy: 0.8501\n","Epoch 19/20\n","391/391 [==============================] - 63s 161ms/step - loss: 0.0119 - accuracy: 0.9968 - val_loss: 0.8050 - val_accuracy: 0.8559\n","Epoch 20/20\n","391/391 [==============================] - 58s 149ms/step - loss: 0.0184 - accuracy: 0.9949 - val_loss: 0.7495 - val_accuracy: 0.8522\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7cc66191cd00>"]},"metadata":{},"execution_count":4}],"source":["# Create the model architecture\n","embedding_vec_length = 128  # Adjust as needed\n","model = Sequential([\n","    Embedding(top_words, embedding_vec_length, input_length=max_review_length),\n","    Dropout(0.2),\n","    LSTM(128),  # Adjust units as needed\n","    Dropout(0.2),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()\n","\n","# Train the model\n","model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=64)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5zO81jAHkOhp","executionInfo":{"status":"ok","timestamp":1717998021262,"user_tz":-330,"elapsed":43054,"user":{"displayName":"Aritra Paul","userId":"06261948115169749460"}},"outputId":"0d40bfc8-7325-4c47-84e7-c0d17d3cf9bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 85.22%\n"]}],"source":["# Evaluate the model's accuracy\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1] * 100))\n","\n"]},{"cell_type":"code","source":["# mount drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LngwQswIkXvU","executionInfo":{"status":"ok","timestamp":1718006523319,"user_tz":-330,"elapsed":25374,"user":{"displayName":"Aritra Paul","userId":"06261948115169749460"}},"outputId":"d3f5ae9d-9a7c-4a1a-b6c3-1f01ac4e45ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Save the model\n","model.save('/content/drive/MyDrive/Senti/sentiment_analysis_model_new.h5')\n","print(\"Saved model to disk\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yXwLIeYekTus","executionInfo":{"status":"ok","timestamp":1717998114567,"user_tz":-330,"elapsed":623,"user":{"displayName":"Aritra Paul","userId":"06261948115169749460"}},"outputId":"d6a37fef-1c13-4e74-e88d-372509ebd4f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved model to disk\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}]},{"cell_type":"code","source":["# Load the model\n","model = load_model('/content/drive/MyDrive/Senti/sentiment_analysis_model_new.h5')\n","# print(\"Model Loaded\")\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wIofUWb1rwOG","executionInfo":{"status":"ok","timestamp":1718006545496,"user_tz":-330,"elapsed":1599,"user":{"displayName":"Aritra Paul","userId":"06261948115169749460"}},"outputId":"f0be37bb-7eaa-42fd-8b7c-cef66d94397a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 2697, 128)         1280000   \n","                                                                 \n"," dropout (Dropout)           (None, 2697, 128)         0         \n","                                                                 \n"," lstm (LSTM)                 (None, 128)               131584    \n","                                                                 \n"," dropout_1 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense (Dense)               (None, 1)                 129       \n","                                                                 \n","=================================================================\n","Total params: 1411713 (5.39 MB)\n","Trainable params: 1411713 (5.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["def preprocess_text(text):\n","  # Lowercase the text\n","  text = text.lower()\n","\n","  # Remove punctuation (optional)\n","  # text = re.sub(r'[^\\w\\s]', '', text)\n","\n","  # Tokenize the text (optional)\n","  # words = text.split()\n","\n","  # ... (add more preprocessing steps if needed)\n","\n","  return text\n"],"metadata":{"id":"m8bmqNW4rp5o"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"peWYUgngkOhp","executionInfo":{"status":"ok","timestamp":1718006589563,"user_tz":-330,"elapsed":1227,"user":{"displayName":"Aritra Paul","userId":"06261948115169749460"}},"outputId":"5d165d1c-3b3f-4cfa-e2f6-1e60d1891b14"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n","1641221/1641221 [==============================] - 0s 0us/step\n","1/1 [==============================] - 1s 839ms/step\n","Sentiment for 'This movie was fantastic!': Negative\n"]}],"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","def predict_sentiment(text):\n","  processed_text = preprocess_text(text)\n","\n","  # Assuming your model expects a sequence of integers representing word IDs\n","  # Load the vocabulary used during training (if necessary)\n","  word_to_id = imdb.get_word_index()  # Assuming vocabulary was saved\n","\n","  # Convert the processed text into a list of word IDs\n","  words = processed_text.split()\n","  x_test = [[word_to_id[word] for word in words if word in word_to_id]]\n","\n","  # Pad the sequence to a fixed length (if needed)\n","  max_review_length = 2697  # Adjust as needed based on your model\n","  x_test = pad_sequences(x_test, maxlen=max_review_length)\n","\n","  # Prepare the input for the model (assuming a single sample)\n","  prediction = model.predict(np.array([x_test[0]]))[0][0]  # Access the first element\n","\n","  if prediction > 0.5:\n","    return \"Positive\"\n","  else:\n","    return \"Negative\"\n","\n","\n","# Example usage\n","text_to_review = \"This movie was fantastic!\"\n","sentiment = predict_sentiment(text_to_review)\n","print(f\"Sentiment for '{text_to_review}': {sentiment}\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"0N38icVwq3p_"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}